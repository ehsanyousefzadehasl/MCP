# FloatPIM: In-Memory Acceleration of Deep Neural Network Trainning with High Precision

[Reference](https://dl.acm.org/doi/10.1145/3307650.3322237)

ISCA - 2019

## What is the problem the paper is trying to solve?
Existing PIM architectures do not support high precision computation, e.g., in floating point precision, which is essential for training accurate CNN models. Additionally, most of the existing PIM approaches require analog/mixed-signal circuits, which do not scale, exploiting insufficiently relaible multi-bit Non-Volatile Memory (NVM).

## What are the key ideas of the paper? Key insights?
Fully-digital scalable PIM architecture
 
## The solution


## Results

## Strenghts

## Weaknesses

## Comment

## New Ideas?

---
## For learning